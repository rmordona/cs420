---
title: "STAT 420, Summer 2018 - Final Project"
date: '15-Jul-2018'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

---

---

## Group Introduction

This study is conducted by the following members from Group 29:

 - Apoorva H Srinivasa (NetID: **apoorva6**)
 - Madhukar K P (NetID: **mk30**)
 - Nicholas Reinicke (NetID: **ndr3**)
 - Raymond Ordona (NetID: **rordona2**)


---

<center><font size="6" color="#002060">**Statistical study of Alcohol By Volume (ABV) in Craft Beer**</font></center>


## Introduction

There are many brew enthusiast who like to brew beer at home. To make a good beer or to improve a recipe over many tries, it is essential to capture several readings during and after the brewing process which requires one to buy costly sensors or measuring gadgets. We wanted to check if we can eliminate any sensors or measuring gadgets that hopefully may suggest cost saving. Particularly we wanted to analyze and see if we can make predictions of one of the parameters namely - ABV - from other readings and do away with any measuring gadgets for it.

As part of this project we intend to study the factors in brewing process that affect the alcohol content in craft beers and also make best predictions on the alcohol content based on the affecting factors. 

---

#### Dataset

For this project, we are using a dataset that contains information on approximately 75,000 different recipes of craft beer along with the readings of various sensors captured during the brewing process. The raw data was sourced from [Brewer's Friend Beer Recipes](https://www.kaggle.com/jtrofe/beer-recipes) available under Kaggle platform.

The dataset has 23 attributes. The response variable - `ABV` is a *real* variable. Below are the key attributes (predictors) that we have identified and used for our study,

| Attribute     | Domain      | Description                                                                    |
|---------------|-------------|--------------------------------------------------------------------------------|
| `SugarScale`  | Categorical | Scale to determine the concentration of dissolved solids in wort |
| `BrewMethod`  | Categorical | Various techniques for brewing (Ex: All Grain) |
| `BoilTime`    | Real        | Time wort is boiled |
| `BoilGravity` | Real        | Specific gravity of wort before the boil |
| `Color`       | Real        | Color using Reference Method - light to dark ex. 40 = black |
| `Efficiency`  | Real        | Beer mash extraction efficiency - extracting sugars from the grain during mash |
| `OG_Specific`  | Real        | Specific gravity of wort before fermentation |
| `FG_Specific`  | Real        | Specific gravity of wort after fermentation |

#### Sample Data

Below is an example of the dataset. Note that 'Style' and 'StyleID' are not part of predictors as they are used more for annotation.


|	  | Style              | StyleID | OG	   | FG	   | IBU   | Color |	Efficiency	| BoilGravity	| BoilTime	| SugarScale	| BrewMethod | ABV
|---|--------------------|--------:|------:|------:|------:|------:|-------------:|-----------:|----------:|-------------|------------|---:
| 1	| Cream Ale          |  45     | 1.055 | 1.013 | 17.65 | 4.83	 | 70	          | 40      |	75	| Specific Gravity |	All Grain | `5.48`
| 5	| Belgian Blond Ale  |  20	   | 1.060 | 1.010 | 17.84 | 4.57	 | 72	          | 52	    | 90	| Specific Gravity |	All Grain | `6.48`
| 6	| American Pale Ale	 |  10	   | 1.055 | 1.013 | 40.12 | 8.00	 | 79	          | 49	    | 70	| Specific Gravity |	All Grain | `5.58`
| 8	| Cream Ale	         |  45	   | 1.054 | 1.014 | 19.97 | 5.94	 | 70	          | 42	    | 75	| Specific Gravity |	All Grain | `5.36`
| 9	| Robust Porter	     | 129	   | 1.060 | 1.016 | 31.63 | 34.76 | 73	          | 44	    | 75	| Specific Gravity |	All Grain | `5.77`


---

## Methods

Our study starts by performing data preparation work. First, we have omitted observations that have missing values for the response - ABV - and one of the predictors - BoilGravity. Secondly, we have converted BoilGravity from categorical to numeric/continuous. Lastly, we have split the 73.8k observations into two sets:  one for training a model, the other for testing the model. This will be useful in the discussion section where we will attempt to predict the alcohol content ( by volume ) of beers from the test set, pretending that we have brewed them at home.

Here, we will be using a 10-k fold validation or a 1-out validation. The train_set has 63.7K randomly sampled observations (90%) and the test_set has the rest of random observations, 7k (10%), using a random seed of 142.


```{r}
library("leaps")
options(scipen = 1, digits = 3, width = 80, fig.align = "center")
```


```{r}

beer = read.csv('recipeData3.csv')
beer$Style = as.factor(beer$Style)


coln = c('OG', 'FG', 'ABV', 'IBU', 'Color', 'Efficiency',
         'BoilGravity',  'BoilTime', 'SugarScale', 'BrewMethod')


dataset = subset( beer[, coln], !BoilGravity %in% c("NA","N/A" ) & !ABV %in% c("NA","N/A"))
dataset[, c("BoilGravity")] = as.numeric(dataset[, c("BoilGravity")])


set.seed(142)
n = nrow(dataset)
ind = sample(1:nrow(dataset), n * 9/10, replace = FALSE, prob = NULL)

train_set = dataset[ind,]
test_set = dataset[-ind,]

nrow(beer)
nrow(train_set)
nrow(test_set)

```

One important note: while there is no sign of collinearity between ABV and FG, we have decided to exclude FG in our models as ABV is directly derived from FG. We will also exclude IBU (which has to do with measuring bitterness of beer)

```{r}
pairs(train_set[, c("ABV", "FG")])
```


Of the methods we have tested, we have narrowed our efforts down to using a.) AIC as criteria in selecting a model and comparing both the STEPWISE search method; and b.) the EXHAUSTIVE search method.

**Also, we have chosen to take three studies in this project:**

Note that all our test excluded FG ( we could also have removed that in our data preparation step).

**Study 1:** Using AIC backward Stepwise Search with **additive model** and **two-way interaction** across the additive model.

* ABV ~ (. -FG - IBU )^2

**Study 2:** Using AIC backward Stepwise Search with **additive model** and **two-way interaction** for only the categorical predictors.

* ABV ~ (. - FG - IBU) +  (OG + Color + Efficiency + BoilGravity + BoilTime ) * (SugarScale + BrewMethod)

**Study 3:** Using Exhaustive Search and **Parameterization (with Dummy Variables)**.

* ABV ~ (. -FG - IBU)^2

The following helper function (**residual_fitted_plot**) is used to plot residual-vs-fitted and outliers.  This function accepts model, data, outlier, and threshold as input parameters. The outlier parameter is BOOLEAN. If set to TRUE, it will use the threshold parameter to compute and plot for the outliers. The function will return the data set ( without the outliers if the outlier parameter and threshold input are used).

```{r}

# This function plots the residual-to-fitted chart. 
# If outlier is set to TRUE, it will return dataset without the outliers;
# otherwise, it returns the complete dataset.
residual_fitted_plot = function(model, data,  outlier, threshold = 0) {
  
  plot(fitted(model), resid(model), xlab="Fitted Values", ylab="Residual", 
       main="Alcohol By Volume (Residual-vs-Fitted Plot)", col="dodgerblue")
  abline(h=0,  col = "darkorange", lwd=2)

  if (outlier == TRUE ) {
    # Plotting outliers
    outliers = resid(model) < threshold
    outlier_index = which( outliers )
    fitted_outliers = fitted(model)[outlier_index]
    resid_outliers = resid(model)[outlier_index]
    
    points(fitted_outliers, resid_outliers, lwd=2, col="red")
    text(fitted_outliers, resid_outliers, labels = round(outlier_index, 3), pos = 2)
    
    #Return dataset without the outliers
    return ( data[-outlier_index,] )
    
  }
  
  return ( data )
}

```

The following helper function (**result_metrics**) is used to capture R2, adjusted R2, RMSE, number of coefficients used, and  list of coefficients that are significant based of a given alpha.

```{r}

# This function returns numeric results (R2, adjusted R3, RMSE, no of coefficients, no of
# non-significant coefficients.)
result_metrics = function(model, alpha) {
  # Get R2  
  model_r2 = round( summary(model)$r.squared, 2)
  # Get Adjusted R2
  model_adj_r2 = round( summary(model)$adj.r.squared, 2)
  # Get RMSE
  model_rmse = round( sqrt(mean(resid(model)^2)), 2 )
  # Get number of coefficients that are significant at .1
  coef = summary(model)$coefficients
  model_coef_cnt = nrow(coef) - 1
  model_not_signif_coef = names( which( coef[,"Pr(>|t|)"] > alpha ) )
  model_not_sigif_coef_cnt = length(model_not_signif_coef)
  list( "r2" = model_r2,
        "adjusted_r2" = model_adj_r2,
        "rmse" = model_rmse,
        "coef_count" = model_coef_cnt,
        "not_sig_coef_count" = model_not_sigif_coef_cnt,
        "not_sig_coef" = model_not_signif_coef )
}
```


---

#### Study 1: Using AIC backward Stepwise Search with additive model and two-way interaction across the additive model.

In this study 1, we have captured the time it takes to capture the search.

```{r}


start_model = lm(ABV ~ (. -FG - IBU)^2 , data = train_set)

system.time((
model1_srch = step( start_model, direction = "backward", trace=0) 
))

```

Note: running the search takes approximately between 27 seconds to 2 minutes.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model1_srch, train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the aic backward stepwise search.

```{r}
model1  =  lm(formula = ABV ~ OG + Color + Efficiency + BoilGravity + BoilTime + 
    SugarScale + BrewMethod + OG:Efficiency + OG:BoilTime + OG:SugarScale + 
    OG:BrewMethod + Color:Efficiency + Color:BoilGravity + Color:BoilTime + 
    Color:SugarScale + Color:BrewMethod + Efficiency:BoilTime + 
    Efficiency:SugarScale + Efficiency:BrewMethod + BoilGravity:BoilTime + 
    BoilGravity:BrewMethod + BoilTime:SugarScale + BoilTime:BrewMethod + 
    SugarScale:BrewMethod, 
    data = data_no_outliers)

model1_result = result_metrics(model1, alpha=0.10)
model1_call = model1$call
model1_result
```

---

#### Study 2: Using AIC backward Stepwise Search with additive model and two-way interaction for only the categorical predictors.

```{r}

start_model = lm(ABV ~ (. - FG - IBU ) + 
        (OG + Color + Efficiency + BoilGravity + BoilTime ) * (SugarScale + BrewMethod),
         data = train_set)

system.time((
model2_srch = step( start_model, direction = "backward", trace=0) 
))

```

Note: running the search takes approximately between 2.829 seconds to 5 seconds.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model2_srch, train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the aic backward stepwise search.

```{r}
model2 =   lm(formula = ABV ~ OG + Color + Efficiency + BoilGravity + BoilTime + 
    SugarScale + BrewMethod + OG:SugarScale + OG:BrewMethod + 
    Color:SugarScale + Efficiency:BrewMethod + BoilGravity:SugarScale + 
    BoilGravity:BrewMethod + BoilTime:SugarScale + BoilTime:BrewMethod, 
    data = data_no_outliers)

model2_result = result_metrics(model2, alpha=0.10)
model2_call = model2$call
model2_result
```


---

#### Study 3: Using Exhaustive Search and Parameterization.

```{r}

system.time( (
start_model = summary(regsubsets(ABV ~ (. -FG - IBU )^2 , data = train_set, really.big=TRUE))
)
)

# Derive coefficients/variables based on the highest Adjusted R-squared.
best_r2_ind = which.max(start_model$adjr2)
coef = names( which(start_model$which[best_r2_ind, ]) )
cat("Chosen variables by Exhaustive Search:\n")
coef
# Parameterization
# Using the coefficients/variables derived from the exhaustive search, create a new dataframe using
# three new dummy variables for both train_set and test_set

new_train_set = data.frame(
ABV = train_set$ABV,
OG = train_set$OG,
Color = train_set$Color,
BoilGravity = train_set$BoilGravity,
BoilTime = train_set$BoilTime,
v1 = 1 * as.numeric(train_set$SugarScale == "Specific Gravity"),
v2 = 1 * as.numeric(train_set$BrewMethod == "BIAB"),
v3 = 1 * as.numeric(train_set$BrewMethod == "extract")
)

new_test_set = data.frame(
ABV = test_set$ABV,
OG = test_set$OG,
Color = test_set$Color,
BoilGravity = test_set$BoilGravity,
BoilTime = test_set$BoilTime,
v1 = 1 * as.numeric(test_set$SugarScale == "Specific Gravity"),
v2 = 1 * as.numeric(test_set$BrewMethod == "BIAB"),
v3 = 1 * as.numeric(test_set$BrewMethod == "extract")
)


# Now try to fit a model
model_exhaust = lm( ABV ~ OG + Color + v1 + v2 + v3 + OG:v1 + Color:BoilTime +  BoilTime:v3,
              data = new_train_set)


```

Note: running the search takes approximately between 0.957 seconds to 5 seconds.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model_exhaust, new_train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the exhaustive search.

```{r}

model3 = lm( ABV ~ OG + Color + v1 + v2 + v3 + OG:v1 + Color:BoilTime +  BoilTime:v3,
              data = data_no_outliers)

model3_result = result_metrics(model3, alpha=0.10)
model3_call = model3$call
model3_result
```


---

## Results

*Summary:* Model 3 is chosen to be the best.  Explained below ...

This study has collected numerical results for R2, adjusted R2, RMSE, number of significant coefficients for comparison. Below is a table listing the results of the three studies:

```{r}
results = NULL
results = rbind(results, model1_result)
results = rbind(results, model2_result)
results = rbind(results, model3_result)
results = as.matrix( results )
results1 = results[, -c(6)]
dframe_results = data.frame(results1)
rownames(dframe_results) = c("Study 1", "Study 2", "Study 3")
knitr::kable(dframe_results, type="markdown")
```

We have shown in our study three good models with R-squared at (`r model3_result$r2`) and 
adjusted-R2 (`r model3_result$adjusted_r2`) and an RMSE (`r model1_result$rmse`). The only difference is the number of variables/coefficients ( showing both significant and non-significant p-values ).
From the  perspective of significance of regression,  it does show that model 3 has the more simple and less variables than the other two models. However, model 1 seems to have a more dominant significant p-values - this may be true based on the idea that the more predictors/variables we have, the more degrees of freedom, and thus, the higher towards fitting a better model.

To prove the point, what does ANOVA tell us in the F-test?

First, we compared model1 and model2.  Model2 being the Null Hypothesis.

```{r}
anova( model1_srch, model2_srch)
```

*Note that model1_srch is model 1 in anova,  and model2_srch is model 2 in anova.  Model 2 is the Null Hypothesis*

The result of the F-TEST does show that model 1 (Full Hypothesis) is the choice of model since the F-test rejects the null hypothesis. 

Using model 1, we compare that againt model 3 next.

```{r}
anova(model1_srch, model_exhaust)
```

*Note that model1_srch is model 1 in anova,  and model_exhaust is model 2 in anova.  Model 2 is the Null Hypothesis*

The result of the F-TEST does show that model 1 (Full Hypothesis) is the choice of model since the F-test rejects the null hypothesis. However, if one has to compare all the numeric values, apart from model 1 being slow in aic backward keywise search, there is really not much significant difference in terms of R2, adjusted R3, and RMSE. In fact, in can be argued that model 1 may have been chosen due to the fact that model 1 has more significant coefficients (`r model1_result$coef_count - model1_result$not_sig_coef_count`) than model 2 (`r model2_result$coef_count - model2_result$not_sig_coef_count`) or 3 (`r model3_result$coef_count - model3_result$not_sig_coef_count`)

So why model 3 seems to be more preferred when R2, adjusted R3, rmse are all the same among model1,2,3 - and model 1 is chosen by ANOVA? This comes down to the goal of this study which is about COST. Without compromising accuracy/measurements, if we can make beer using the least gadgets and sensors (or categorical choices even), that would be ideal. Basically, why complicate if we can get the same results with less cost. (Ref 17.3 of text).

---

## Discussion

We used new_test_set to predict ABV using model3 ( choice of model) using alpha = 0.01.

```{r}

predicted_interval = predict(model3, new_test_set, interval="prediction", level=0.01)
actual_abv = new_test_set$ABV
predicted_abv = predicted_interval[,"fit"]

abv = seq(0,15, 1)
par(mfrow=c(2,2))
hist(predicted_abv, breaks=100, xlim=range(0,15), xlab="Predicted Value from Model 3", 
     main="ABV Distribution (Predicted)")
hist(actual_abv,  breaks=100, xlim=range(0,15), xlab="Actual value from Test Set", 
     main="ABV Distribution (Actual")
```

Histogram shows similarity between predicted value and actual value. Occasional drinkers most likely prefer ABV between 4-6 which is exactly where the highest frequency falls;  and the frequency pattern of the predicted values fall in the same range.

Additionally, sampling the first 20 ABVs of both actual and predict are very close. 

```{r}
head(actual_abv, 20)
round(as.vector(head(predicted_abv, 20)),2)
```

Looking at predicted values vs actual values

```{r}
plot(predicted_abv, actual_abv, col="dodgerblue", ylab="Actual", 
      xlab="Predicted", main="Actual vs Predicted for Test Data")
abline(0,1, col="darkorange", lwd=2)
```


Checking RMSE:

```{r}
(rmse = sqrt(mean((predicted_abv - actual_abv)^2)))
```

Basically, if we are to assume for a moment that the test data all come from our attempts to brew beer, it would seem that by following the measuring gadgets ( or sensors ) based on the preferred model 3, it can be observed that a standard error of  `r rmse` seems enough to predict the expected alcohol content without other extraneous sensors - and to rely for more additional sensors does not give additional benefit or effect (see R-squared/RMSE) (though statistically, those extraneous sensors may be significant by ANOVA). It should be noted that based on the Exhaustive search (model 3), for ABV (alcohol by volume), by way of parameterization (sort of brute force), it suggest to have a good mix of either "extract" or "BIAB" for BrewMethod along with "Specific Gravity" for SugarScale. The interaction between Color and BoilTime, Boltime and Brewmethod of 'extract'. (This could very well be done also by subsetting dataset to those observations only)

In other words, it is fair to say - referring to some of the extraneous variables - that we don't have to measure the interaction between Color and BoilGravity, or between Color and Efficiency as determined by model 1 because those interactions do not prove to have additional effect in model 3. (Ref 17.3 of text)

**Other analysis in the context of drinking beers:**

Assume, that we are master at brewing beer at home, and all we want is really to serve up occasional (non-strong) drinkers. We know that those beer drinkers may prefer ABV in the range of 4-6; in other words, the predicted-vs-actual plot show some unusual observations above the 20 or even 30 range. Shall we eliminate them? Let's see.

```{r}
range(actual_abv)
range(predicted_abv)
```

Oh no! We got very strong beer mix out there!  If we are to serve up beer between 4-6 ABVs, what would that proportion be in the true distribution?

```{r}
mean(actual_abv <= 6 & actual_abv >=4 ) 
```

Would that be the same in the emperical distribution?

```{r}
mean(predicted_abv <= 6 & predicted_abv >=4 ) 
```

Close enough! Not bad!

---

## Appendix

Some graphs to point out.

This is the result of the dataset after outliers are removed (whether model1, model2, model3).

Residual vs Fitted Plot:

```{r}
data_no_outliers =  residual_fitted_plot(model3, new_train_set, FALSE) 
```

Notice that residual range dropped from (-19, 6) to (-6, to 6)

Normal Q-Q Plot:

```{r}
# Let's draw Normal Q-Q plot for the original model3
qqnorm(resid(model3), col = "darkgrey")
qqline(resid(model3), col = "dodgerblue", lwd = 2)
```

Still within the range of the residual range. The residuals are normally distributed (symmetric), but we are not going to put penalty on the outliers ( tails moving away from the blue line).

---

**Just an extra, for fun,  what if we brew our own beer at home, but want to only serve 4-6 ABV contents for occasional beer drinkers?**

Let's see (still using model 3):

```{r}

test_for_occasions = new_test_set[ which ( new_test_set$ABV <= 6 & new_test_set$ABV >=4 ),]

predicted_occasions = predict(model3, test_for_occasions, type="response")
actual_occasions = test_for_occasions$ABV

# RMSE
(rmse = sqrt(mean((predicted_occasions - actual_occasions)^2)))

```

That's so much better. We should be able to brew beer, predicting an expected alcohool content with an error of only `r rmse` if we focus our beer making around the 4-6 ABV.

**Can we serve drinkers with advance taste for beer?**

Here, we're assuming they're looking for stronger beer above 6 ABV perhaps. Let's see:

```{r}

test_for_hard = new_test_set[ which ( new_test_set$ABV > 6 ),]

predicted_hard = predict(model3, test_for_hard, type="response")
actual_hard = test_for_hard$ABV

# RMSE
(rmse = sqrt(mean((predicted_hard - actual_hard)^2)))

```

Probably not. We will have an error rate of `r rmse` - those beer drinkers will not be happy.

---

